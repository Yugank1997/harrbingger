{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yugan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yugan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yugan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing all the relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from googletrans import Translator\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import text2emotion as te\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import dash\n",
    "import plotly.express as px  \n",
    "from dash import Dash, dcc, html, Input, Output, dash_table, State, callback\n",
    "import dash_bootstrap_components as dbc\n",
    "from skimage import io\n",
    "import snscrape\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yugan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yugan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "today1=today.strftime('%Y-%m-%d')\n",
    "last_week= today - timedelta(days=30)\n",
    "last_week=last_week.strftime('%Y-%m-%d')\n",
    "today=today.strftime(\"%m/%d/%Y\")\n",
    "today=today.replace(\"/\",\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Media Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the file that contains the keywords for which the tweets will be extracted\n",
    "df_terms = pd.read_csv ('tweet_terms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to take out all the tweets\n",
    "j=0\n",
    "df_finalscrap=pd.DataFrame()\n",
    "while j<len(df_terms.Terms):\n",
    "    tweets_list= []\n",
    "    term= df_terms.Terms[j]\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(term +' since:'+last_week+' until:'+today1+' near: India').get_items()):\n",
    "        if i>100:\n",
    "            break\n",
    "        tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.user.location, tweet.lang, tweet.source])\n",
    "    # Creating a dataframe from the tweets list above\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username', 'Location', 'Language', 'Source'])\n",
    "    tweets_df['Keyword']=term\n",
    "    df_finalscrap= pd.concat([df_finalscrap, tweets_df])\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_clean_terms = pd.read_csv ('tweet_clean_terms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove junk tweets\n",
    "i=0\n",
    "while i<len(df_finalscrap.Text):\n",
    "    j=0\n",
    "    while j<len(df_tweet_clean_terms.Delete_term):\n",
    "        delete_term= df_tweet_clean_terms.Delete_term[j]\n",
    "        j+=1\n",
    "        if (df_finalscrap.iloc[i,2]).find(delete_term)>-1:\n",
    "            df_finalscrap=df_finalscrap.drop(df_finalscrap.index[i])\n",
    "            i-=1\n",
    "            break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finalscrap.to_csv(\"Scrapped_tweets_\" + today +\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Location</th>\n",
       "      <th>Language</th>\n",
       "      <th>Source</th>\n",
       "      <th>Keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-04-06 15:46:17+00:00</td>\n",
       "      <td>1511732029703946244</td>\n",
       "      <td>America had stormy Daniel's,  India had Stormy...</td>\n",
       "      <td>b_libeesh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-25 15:56:46+00:00</td>\n",
       "      <td>1507386015597883393</td>\n",
       "      <td>National Payments Commission of India (#NPCI) ...</td>\n",
       "      <td>Opinionmint</td>\n",
       "      <td>India</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>upi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   Datetime             Tweet Id  \\\n",
       "0           1  2022-04-06 15:46:17+00:00  1511732029703946244   \n",
       "1           1  2022-03-25 15:56:46+00:00  1507386015597883393   \n",
       "\n",
       "                                                Text     Username Location  \\\n",
       "0  America had stormy Daniel's,  India had Stormy...    b_libeesh      NaN   \n",
       "1  National Payments Commission of India (#NPCI) ...  Opinionmint    India   \n",
       "\n",
       "  Language                                             Source Keyword  \n",
       "0       en  <a href=\"http://twitter.com/download/android\" ...   Fraud  \n",
       "1       en  <a href=\"http://twitter.com/download/android\" ...     upi  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_raw = pd.read_csv (\"Scrapped_tweets_\" + today +\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translating all tweets to english\n",
    "i=0\n",
    "l=len(df_tweets_raw.Text)\n",
    "translated_tweets = []\n",
    "while i<l:\n",
    "    try:\n",
    "        temp= translator.translate(df_tweets_raw.Text[i], dest='en')\n",
    "        translated_tweets.append(temp.text)\n",
    "    except:\n",
    "        translated_tweets.append(df_tweets_raw.Text[i])\n",
    "    # to check if it is iterating or not\n",
    "    #print(i)  \n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_raw['Translated']=translated_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# removing useless columns\n",
    "df_tweets_raw.drop('Unnamed: 0', axis='columns', inplace=True)\n",
    "df_tweets_raw.drop('Tweet Id', axis='columns', inplace=True)\n",
    "df_tweets_raw.drop('Source', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "df_tweets_raw.to_csv('translated_tweets_' + today +'.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_translated = df_tweets_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case function\n",
    "def lowertext(text):\n",
    "    x=text.lower()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert list to string\n",
    "def listToString(s):  \n",
    "    str1 = \"\" \n",
    "    for ele in s: \n",
    "        str1 += ele + \" \"  \n",
    "    return str1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## email id extraction\n",
    "def email_ext(text):\n",
    "    email = re.findall('\\S+@\\S+', text)\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## email id removal\n",
    "def email_rem(text):\n",
    "    email = re.sub('\\S+@\\S+','', text)\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## phone no extraction\n",
    "def phoneno_ext(text):\n",
    "    phoneno= re.findall('((?:\\+\\d{2}[-\\.\\s]??|\\d{4}[-\\.\\s]??)?(?:\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}))', text)\n",
    "    return phoneno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## phone no removal\n",
    "def phoneno_rem(text):\n",
    "    phoneno= re.sub('((?:\\+\\d{2}[-\\.\\s]??|\\d{4}[-\\.\\s]??)?(?:\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}))','' ,text)\n",
    "    return phoneno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal of https, urls and other links\n",
    "def urlbuster(thestring):\n",
    "    URLless_string = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', thestring)\n",
    "    return(URLless_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction of hashtags\n",
    "def extract_hashtags(text):\n",
    "    hashtag_list = []\n",
    "    for word in text.split():\n",
    "        if word[0] == '#':\n",
    "            hashtag_list.append(word[1:])\n",
    "    return(hashtag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of hashtags\n",
    "def remove_hashtags(text):\n",
    "    hashtag_list = []\n",
    "    for word in text.split():\n",
    "        if word[0] != '#':\n",
    "            hashtag_list.append(word[0:])\n",
    "    return(listToString(hashtag_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction of @ mentions\n",
    "def extract_mention(text):\n",
    "    mention_list = []\n",
    "    for word in text.split():\n",
    "        if word[0] == '@':\n",
    "            mention_list.append(word[1:])\n",
    "    return(mention_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of @ mentions\n",
    "def remove_mention(text):\n",
    "    mention_list = []\n",
    "    for word in text.split():\n",
    "        if word[0] != '@':\n",
    "            mention_list.append(word[0:])\n",
    "    return(listToString(mention_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of accented characters \n",
    "def accento_correcto(text):\n",
    "    output_string = unidecode.unidecode(text)\n",
    "    return(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, punctuations\n",
    "def not_so_special_anymore(text):\n",
    "    x=re.sub('[^A-Za-z0-9 ]+', ' ', text)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove recurring spaces\n",
    "def singularity(text):\n",
    "    x=re.sub(' +', ' ', text)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tweets_translated.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling functions for extraction of email id, phone no, hashtags, mentions and removal of everything except stopwords\n",
    "# Entity dealt seperately\n",
    "i=0\n",
    "l=len(df_tweets_translated.Translated)\n",
    "cleaned_data = []\n",
    "email_list=[]\n",
    "phonebook=[]\n",
    "hashtag_list =[]\n",
    "mention_list=[]\n",
    "while i<l:\n",
    "    try:\n",
    "        email_list.append(email_ext(df_tweets_translated.Translated[i]))\n",
    "    except:\n",
    "        email_list.append(\"\")\n",
    "    try:\n",
    "        phonebook.append(phoneno_ext(df_tweets_translated.Translated[i]))\n",
    "    except:\n",
    "        phonebook.append(\"\")\n",
    "    try:\n",
    "        mention_list.append(extract_mention(df_tweets_translated.Translated[i]))\n",
    "    except:\n",
    "        mention_list.append(\"\")\n",
    "    try:    \n",
    "        hashtag_list.append(extract_hashtags(df_tweets_translated.Translated[i]))\n",
    "    except:\n",
    "        hashtag_list.append(\"\")\n",
    "     \n",
    "    temp=(accento_correcto(df_tweets_translated.Translated[i]))\n",
    "    temp=(remove_mention(temp))\n",
    "    temp=(remove_hashtags(temp))\n",
    "    temp=(phoneno_rem(temp))\n",
    "    temp=(email_rem(temp))\n",
    "    temp=(urlbuster(temp))\n",
    "    temp=(singularity(temp))\n",
    "    temp=(not_so_special_anymore(temp))\n",
    "    cleaned_data.append(temp)          \n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\spacy\\util.py:833: UserWarning: [W095] Model 'en_core_web_sm' (3.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.2.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "NER = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take out the entities\n",
    "def inspector_entity_detector(string):\n",
    "    text1=NER(string)\n",
    "    temp=[]\n",
    "    for word in text1.ents:\n",
    "        if word.label_ == \"GPE\" or word.label_== \"PERSON\" or word.label_== \"ORG\":\n",
    "            temp.append(word.text)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function to extract the entities\n",
    "i=0\n",
    "l=len(cleaned_data)\n",
    "entity_set = []\n",
    "while i<l:\n",
    "    temp=[]\n",
    "    try:\n",
    "        temp= inspector_entity_detector(cleaned_data[i])\n",
    "    except:\n",
    "        temp=None\n",
    "    entity_set.append(temp)\n",
    "    #print(i)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_translated['Entities']=entity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting entity superset list of list to list for removal from tweets\n",
    "entity_superlist=[]\n",
    "l=len(entity_set)\n",
    "i=0\n",
    "while i<l:\n",
    "    k=len(entity_set[i])\n",
    "    j=0\n",
    "    while j<k:\n",
    "        temp=entity_set[i][j].lower()\n",
    "        entity_superlist.append(temp)\n",
    "        j+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting entity superset list of list to list for word cloud later\n",
    "entity_superlist_cloud=[]\n",
    "l=len(entity_set)\n",
    "i=0\n",
    "while i<l:\n",
    "    k=len(entity_set[i])\n",
    "    j=0\n",
    "    while j<k:\n",
    "        temp=entity_set[i][j].lower()\n",
    "        temp=re.sub(' +', ' ', temp)\n",
    "        temp=(temp.strip())\n",
    "        entity_superlist_cloud.append(temp)\n",
    "        j+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of entitites to dictionary with frequency without repetition\n",
    "freq = {} \n",
    "for item in entity_superlist_cloud: \n",
    "    if (item in freq): \\\n",
    "        freq[item] += 1\n",
    "    else: \n",
    "        freq[item] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function removal of entities from tweets\n",
    "def entity_buster(text):\n",
    "    text_ls= text.split() \n",
    "    final_list = [x for x in text_ls if x not in entity_superlist]\n",
    "    final_string= ' '.join(final_list)\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal of entities and case conversion\n",
    "i=0\n",
    "l=len(cleaned_data)\n",
    "final_data = []\n",
    "while i<l:\n",
    "    \n",
    "    try:\n",
    "        temp= entity_buster(cleaned_data[i])\n",
    "        final_data.append(lowertext(temp))\n",
    "    except:\n",
    "        final_data.append(lowertext(cleaned_data[i]))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tweets_translated.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_translated['Email']=email_list\n",
    "df_tweets_translated['Phone_no']=phonebook\n",
    "df_tweets_translated['Hashtag']=hashtag_list\n",
    "df_tweets_translated['Mentions']=mention_list\n",
    "df_tweets_translated['data_w_stopwords']=final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_tweets_translated['Language']\n",
    "del df_tweets_translated['Translated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into 2 based on stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword removal\n",
    "sw_spacy = NER.Defaults.stop_words\n",
    "def stopword_remover(text):\n",
    "    words = [word for word in text.split() if word.lower() not in sw_spacy]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy word tokenizer\n",
    "def token_of_appreciation(texts):\n",
    "    doc=NER(texts)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        tokens.append(token.text)\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemon_lemmatizer(texts):\n",
    "    # Tokenize: Split the sentence into words\n",
    "    word_list = nltk.word_tokenize(texts)\n",
    "    # Lemmatize list of words and join\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal of stopwords\n",
    "i=0\n",
    "l=len(df_tweets_translated.data_w_stopwords)\n",
    "wo_stopwords = []\n",
    "while i<l:\n",
    "    try:\n",
    "        wo_stopwords.append(stopword_remover(df_tweets_translated.data_w_stopwords[i]))\n",
    "    except:\n",
    "        wo_stopwords.append(df_tweets_translated.data_w_stopwords[i])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_translated['data_wo_stopwords']=wo_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis and Emotion Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagger dictionary\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "df_tweets_translated['POS tagged'] = df_tweets_translated['data_w_stopwords'].apply(token_stop_pos)\n",
    "#df_tweets_translated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing words to their lemmas\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "df_tweets_translated['Lemma'] = df_tweets_translated['POS tagged'].apply(lemmatize)\n",
    "#df_tweets_translated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_data = pd.DataFrame(df_tweets_translated[['Text', 'Lemma']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the sentiments\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# function to calculate vader sentiment\n",
    "def darthvadersentimentanalysis(review):\n",
    "    vs = analyzer.polarity_scores(review)\n",
    "    return vs['compound']\n",
    "\n",
    "fin_data['Vader Sentiment'] = fin_data['Lemma'].apply(darthvadersentimentanalysis)\n",
    "# function to analyse\n",
    "def darthvader_analysis(compound):\n",
    "    if compound >= 0.5:\n",
    "        return 'Positive'\n",
    "    elif compound <= -0.5 :\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "fin_data['Vader Analysis'] = fin_data['Vader Sentiment'].apply(darthvader_analysis)\n",
    "#fin_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_counts = fin_data['Vader Analysis'].value_counts()\n",
    "#vader_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotional analysis\n",
    "# removal of stopwords\n",
    "i=0\n",
    "l=len(df_tweets_translated.data_w_stopwords)\n",
    "emotions = []\n",
    "while i<l:\n",
    "    q=te.get_emotion(df_tweets_translated.data_w_stopwords[i])\n",
    "    k = Counter(q)\n",
    "    high = k.most_common(2)\n",
    "    emotions.append(high)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_data['Emotions']=emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_translated['Sentiment']=fin_data['Vader Analysis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_translated['Emotion']=emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_tweets_translated['Lemma']\n",
    "del df_tweets_translated['POS tagged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "df_tweets_translated.to_csv('Analysis_file_' + today +'.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to be included in Interactive Dashboard\n",
    "1. EDA (Exploratory data analysis)\n",
    "    1.1 Total no of tweets analysed in the run\n",
    "    1.2 Geographic distribution\n",
    "2. Type of fraud/ payment system in question\n",
    "3. Overall sentiment (Vader graph)\n",
    "4. Entity word cloud (after removing geographical locations)\n",
    "5. Emotion superset pie chart\n",
    "6. Button see historical entity data and see changes\n",
    "7. Button to see the complete final analysed data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Location</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Email</th>\n",
       "      <th>Phone_no</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>data_w_stopwords</th>\n",
       "      <th>data_wo_stopwords</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-13 11:16:05+00:00</td>\n",
       "      <td>@kailash29664024 @DanielSamsDolan Yes in 2016 ...</td>\n",
       "      <td>seculariscringe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fraud</td>\n",
       "      <td>['Australia', 'india', 'WTC']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['2966402']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['kailash29664024', 'DanielSamsDolan']</td>\n",
       "      <td>yes in 2016 but his performance in australia a...</td>\n",
       "      <td>yes 2016 performance australia series wtc fina...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[('Sad', 0.5), ('Fear', 0.5)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Datetime  \\\n",
       "0  2022-03-13 11:16:05+00:00   \n",
       "\n",
       "                                                Text         Username  \\\n",
       "0  @kailash29664024 @DanielSamsDolan Yes in 2016 ...  seculariscringe   \n",
       "\n",
       "  Location Keyword                       Entities Email     Phone_no Hashtag  \\\n",
       "0      NaN   Fraud  ['Australia', 'india', 'WTC']    []  ['2966402']      []   \n",
       "\n",
       "                                 Mentions  \\\n",
       "0  ['kailash29664024', 'DanielSamsDolan']   \n",
       "\n",
       "                                    data_w_stopwords  \\\n",
       "0  yes in 2016 but his performance in australia a...   \n",
       "\n",
       "                                   data_wo_stopwords Sentiment  \\\n",
       "0  yes 2016 performance australia series wtc fina...   Neutral   \n",
       "\n",
       "                         Emotion  \n",
       "0  [('Sad', 0.5), ('Fear', 0.5)]  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_graphing = pd.read_csv ('Analysis_file_' + today +'.csv')\n",
    "df_graphing.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_graphing['data_w_stopwords']\n",
    "del df_graphing['data_wo_stopwords']\n",
    "del df_graphing['Username']\n",
    "del df_graphing['Hashtag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning severity scores to tweets based on detected sentiment and emotions\n",
    "i=0\n",
    "score_list=[]\n",
    "classification_list=[]\n",
    "while i<len(df_graphing.Emotion):\n",
    "    score123=0\n",
    "    if df_graphing.Sentiment[i]==\"Negative\":\n",
    "        score123+=1\n",
    "    elif df_graphing.Sentiment[i]==\"Positive\":\n",
    "        score123-=1\n",
    "    else:\n",
    "        score123+=0\n",
    "\n",
    "    if df_graphing.Emotion[i].find(\"Fear\")>-1:\n",
    "        score123+=2\n",
    "    if df_graphing.Emotion[i].find(\"Sad\")>-1:\n",
    "        score123+=1\n",
    "    if df_graphing.Emotion[i].find(\"Angry\")>-1:\n",
    "        score123+=2\n",
    "    if df_graphing.Emotion[i].find(\"Happy\")>-1:\n",
    "        score123-=1\n",
    "    if df_graphing.Emotion[i].find(\"Suprise\")>-1:\n",
    "        score123+=0\n",
    "    score_list.append(score123)\n",
    "    if score123>2:\n",
    "        classification_list.append(\"Complaint\")\n",
    "    elif score123>-1:\n",
    "        classification_list.append(\"Suggestions & Recommendations\")\n",
    "    else:\n",
    "        classification_list.append(\"Positive Experiences\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graphing['Severity']=score_list\n",
    "df_graphing['Classfication']=classification_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "#totwal no of tweets\n",
    "total_tweet_count= len(df_graphing.Text)\n",
    "\n",
    "# geographical distribution\n",
    "tweets_geo=df_graphing.Location\n",
    "cleanedList = [x for x in tweets_geo if str(x) != 'nan']\n",
    "\n",
    "# convert list of location to dictionary with frequency without repetition\n",
    "freq_geo = {} \n",
    "for item in cleanedList: \n",
    "    if (item in freq_geo): \\\n",
    "        freq_geo[item] += 1\n",
    "    else: \n",
    "        freq_geo[item] = 1\n",
    "\n",
    "counter_geo = Counter(freq_geo)\n",
    "high_geo= counter_geo.most_common(10)\n",
    "high_geo_dict = dict(high_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart for top 10 locations of tweets\n",
    "names_geo = list(high_geo_dict.keys())\n",
    "values_geo = list(high_geo_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of type of fraud to dictionary with frequency without repetition\n",
    "freq_keyword = {} \n",
    "for item in df_graphing.Keyword: \n",
    "    if (item in freq_keyword): \\\n",
    "        freq_keyword[item] += 1\n",
    "    else: \n",
    "        freq_keyword[item] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to plot\n",
    "labels_type = []\n",
    "sizes_type = []\n",
    "\n",
    "for x, y in freq_keyword.items():\n",
    "    labels_type.append(x)\n",
    "    sizes_type.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of Classification to dictionary with frequency without repetition\n",
    "freq_keyword_class = {} \n",
    "for item in df_graphing.Classfication: \n",
    "    if (item in freq_keyword_class): \\\n",
    "        freq_keyword_class[item] += 1\n",
    "    else: \n",
    "        freq_keyword_class[item] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to plot\n",
    "labels_type_class = []\n",
    "sizes_type_class = []\n",
    "\n",
    "for x, y in freq_keyword_class.items():\n",
    "    labels_type_class.append(x)\n",
    "    sizes_type_class.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy of\n",
    "freq_cloud=freq.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads the entity cleaner file and converts into a list\n",
    "df_entity_cleaner = pd.read_csv ('entity_cleaner.csv')\n",
    "entity_cleaner_list=df_entity_cleaner.Remove.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning entities based on list\n",
    "dels = []\n",
    "for k, v in freq_cloud.items():\n",
    "    if k in entity_cleaner_list:\n",
    "        dels.append(k)\n",
    "\n",
    "for i in dels:\n",
    "    del freq_cloud[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating of world cloud and saving it as image for dashboard\n",
    "wordcloud = WordCloud(width = 500, height = 500).generate_from_frequencies(freq_cloud)\n",
    "\n",
    "wordcloud =  wordcloud.to_file('Entity_cloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions pie chart\n",
    "emotion_list_graph=[]\n",
    "emotion_list_graph=df_graphing.Emotion.copy()\n",
    "emotion_string_graph= emotion_list_graph.to_string()\n",
    "emotion_string_graph=re.sub(r'[^a-zA-Z]', ' ', emotion_string_graph)\n",
    "emotion_string_graph=re.sub(' +', ' ', emotion_string_graph)\n",
    "emotions_tokens = nltk.word_tokenize(emotion_string_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of entitites to dictionary with frequency without repetition\n",
    "freq_emotion= {} \n",
    "for item in emotions_tokens: \n",
    "    if (item in freq_emotion): \\\n",
    "        freq_emotion[item] += 1\n",
    "    else: \n",
    "        freq_emotion[item] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to plot\n",
    "labels_emo = []\n",
    "sizes_emo = []\n",
    "\n",
    "for x, y in freq_emotion.items():\n",
    "    labels_emo.append(x)\n",
    "    sizes_emo.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Dash(__name__)\n",
    "# App layout\n",
    "\n",
    "\n",
    "url_bar_and_content_div = html.Div([\n",
    "    dcc.Location(id='url', refresh=False),\n",
    "    html.Div(id='page-content')\n",
    "])\n",
    "\n",
    "layout_index = html.Div([\n",
    "    dcc.Textarea(\n",
    "        id='textarea-example',\n",
    "        value='Select one of the options from below to navigate to either see the data with analysis columns or see the analytical graphs for overview ',\n",
    "        style={'width': '80%', 'height': 50},\n",
    "    ),\n",
    "    html.Div(id='textarea-example-output', style={'whiteSpace': 'pre-line'}),\n",
    "    html.Br(),\n",
    "    dcc.Link('Navigate to Data page', href='/page-1'),\n",
    "    html.Br(),\n",
    "    dcc.Link('Navigate to Analysis', href='/page-2'),\n",
    "])\n",
    "\n",
    "layout_page_1 = html.Div([\n",
    "    html.H2('Data View'),\n",
    "    html.Br(),\n",
    "    dcc.Link('Navigate to \"/\"', href='/'),\n",
    "    html.Br(),\n",
    "    dcc.Link('Navigate to Analysis', href='/page-2'),\n",
    "    html.Br(),\n",
    "    dcc.Textarea(\n",
    "        id='textarea-example',\n",
    "        value='Scroll Right to see all the columns. Scroll to the bottom to go to the next page (50 records per page). You can also sort and filter data from the top.',\n",
    "        style={'width': '80%', 'height': 50},\n",
    "    ),\n",
    "    html.Div(id='textarea-example-output', style={'whiteSpace': 'pre-line'}),\n",
    "    html.Br(),\n",
    "    dash_table.DataTable(\n",
    "        id='datatable-interactivity',\n",
    "        columns=[\n",
    "            {\"name\": i, \"id\": i, \"deletable\": False, \"selectable\": True} for i in df_graphing.columns\n",
    "        ],\n",
    "        data=df_graphing.to_dict('records'),\n",
    "        editable=False,\n",
    "        filter_action=\"native\",\n",
    "        sort_action=\"native\",\n",
    "        sort_mode=\"multi\",\n",
    "        row_deletable=False,\n",
    "        page_action=\"native\",\n",
    "        page_current= 0,\n",
    "        page_size= 50,\n",
    "    ),\n",
    "    html.Div(id='datatable-interactivity-container')\n",
    "])\n",
    "\n",
    "layout_page_2 = html.Div([\n",
    "    html.H1(\"Fraud Detection using Social Media\", style={'text-align': 'center'}),\n",
    "    dcc.Dropdown(id=\"slct_year\",\n",
    "                 options=[\n",
    "                     {\"label\": \"Exploratory Data Analysis\", \"value\": \"Exploratory Data Analysis\"},\n",
    "                     {\"label\": \"Type of Fraud and Payment Instrument\", \"value\": \"Type of Fraud and Payment Instrument\"},\n",
    "                     {\"label\": \"Sentiment Analysis\", \"value\": \"Sentiment Analysis\"},\n",
    "                     {\"label\": \"Emotion Classification\", \"value\": \"Emotion Classification\"},\n",
    "                     {\"label\": \"Entity Detection\", \"value\": \"Entity Detection\"},\n",
    "                     {\"label\": \"Classification of Tweets based on Intent\", \"value\": \"Intent\"}],                     \n",
    "                 multi=False,\n",
    "                 value=\"Intent\",\n",
    "                 style={'width': \"60%\"},\n",
    "                 placeholder=\"Select an analysis\"\n",
    "                 ),\n",
    "    html.Div(id='output_container', children=[]),\n",
    "    html.Br(),\n",
    "    html.Div(id='page-2-display-value'),\n",
    "    html.Br(),\n",
    "    dcc.Link('Navigate to \"/\"', href='/'),\n",
    "    html.Br(),\n",
    "    dcc.Link('Navigate to Data page', href='/page-1'),\n",
    "    html.Br(),\n",
    "    dcc.Graph(id='my_bee_map', figure={})\n",
    "])\n",
    "\n",
    "# index layout\n",
    "app.layout = url_bar_and_content_div\n",
    "\n",
    "# \"complete\" layout\n",
    "app.validation_layout = html.Div([\n",
    "    url_bar_and_content_div,\n",
    "    layout_index,\n",
    "    layout_page_1,\n",
    "    layout_page_2,\n",
    "])\n",
    "\n",
    "\n",
    "# Index callbacks\n",
    "@app.callback(Output('page-content', 'children'),\n",
    "              Input('url', 'pathname'))\n",
    "def display_page(pathname):\n",
    "    if pathname == \"/page-1\":\n",
    "        return layout_page_1\n",
    "    elif pathname == \"/page-2\":\n",
    "        return layout_page_2\n",
    "    else:\n",
    "        return layout_index\n",
    "\n",
    "\n",
    "# Page 1 callbacks\n",
    "@app.callback(\n",
    "    Output('datatable-interactivity', 'style_data_conditional'),\n",
    "    Input('datatable-interactivity', 'selected_columns')\n",
    ")\n",
    "def update_styles(selected_columns):\n",
    "    return [{\n",
    "        'if': { 'column_id': i },\n",
    "        'background_color': '#D2F3FF'\n",
    "    } for i in selected_columns]\n",
    "\n",
    "\n",
    "# Page 2 callbacks\n",
    "@app.callback(\n",
    "    [Output(component_id='output_container', component_property='children'),\n",
    "     Output(component_id='my_bee_map', component_property='figure')],\n",
    "    [Input(component_id='slct_year', component_property='value')]\n",
    ")\n",
    "def update_graph(option_slctd):\n",
    "    print(option_slctd)\n",
    "    print(type(option_slctd))\n",
    "    \n",
    "    container = \"The analysis chosen is: {}\".format(option_slctd)\n",
    "    \n",
    "    #fig = px.pie(df_graphing, values=vader_counts.values, names= vader_counts.index)\n",
    "\n",
    "    \n",
    "    if option_slctd == 'Emotion Classification':\n",
    "        fig = px.pie(df_graphing, values=sizes_emo, names=labels_emo)    \n",
    "\n",
    "    elif option_slctd == 'Exploratory Data Analysis':\n",
    "        fig = px.pie(df_graphing, values=values_geo, names=names_geo)    \n",
    "    \n",
    "    elif option_slctd == 'Type of Fraud and Payment Instrument':\n",
    "        fig = px.pie(df_graphing, values=sizes_type, names=labels_type)\n",
    "        \n",
    "    elif option_slctd == 'Intent':\n",
    "        fig = px.pie(df_graphing, values=sizes_type_class, names=labels_type_class)\n",
    "       \n",
    "    elif option_slctd == 'Sentiment Analysis':\n",
    "        fig = px.pie(df_graphing, values=vader_counts.values, names= vader_counts.index)\n",
    "  \n",
    "    elif option_slctd == 'Entity Detection':\n",
    "        img = io.imread('Entity_cloud.png')\n",
    "        fig = px.imshow(img)\n",
    "        \n",
    "    return container, fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:13] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:13] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:13] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:13] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:14] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:14] \"\u001b[36mGET /_dash-component-suites/dash/dash_table/async-highlight.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:14] \"\u001b[36mGET /_dash-component-suites/dash/dash_table/async-table.js HTTP/1.1\u001b[0m\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\dash.py\", line 1383, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-109-281df6885211>\", line 111, in update_styles\n",
      "    return [{\n",
      "TypeError: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Apr/2022 05:07:14] \"\u001b[35m\u001b[1mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 500 -\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:28] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:28] \"\u001b[36mGET /_dash-component-suites/dash/dcc/async-dropdown.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:28] \"\u001b[36mGET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:28] \"\u001b[36mGET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [08/Apr/2022 05:07:28] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
